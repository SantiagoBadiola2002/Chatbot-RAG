from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from llama_index.core import SQLDatabase
from llama_index.core.query_engine import NLSQLTableQueryEngine
import os
import logging
from sqlalchemy import create_engine, inspect
from router import detectar_intencion 

# ==============================
# üîë Configuraci√≥n
# ==============================
logging.basicConfig(level=logging.WARNING)
os.environ["GOOGLE_API_KEY"] = "TU_API_KEY_AQUI"  # Reemplaza con tu clave real

# ==============================
# ‚öôÔ∏è Conexi√≥n a MySQL
# ==============================
engine = create_engine("mysql+pymysql://root:@localhost/tracking_db")
sql_database = SQLDatabase(engine)

# Obtener autom√°ticamente todas las tablas de la DB
inspector = inspect(engine)
tablas_db = inspector.get_table_names()  # ‚úÖ devuelve lista de strings directamente


# ==============================
# Modelo LLM
# ==============================
llm = GoogleGenAI(model="gemini-2.0-flash")

# ==============================
# ‚öôÔ∏è Configuraci√≥n Qdrant
# ==============================
qdrant_client = QdrantClient(
    url="TU_QDRANT_URL_AQUI",  # Reemplaza con tu URL real
    api_key="TU_QDRANT_API_KEY_AQUI"  # Reemplaza con tu API Key real
)

embed_model = GoogleGenAIEmbedding(
    model_name="text-embedding-004",
    embed_batch_size=100
)

vector_store = QdrantVectorStore(
    client=qdrant_client,
    collection_name="docs_collection"
)

storage_context = StorageContext.from_defaults(vector_store=vector_store)

# ==============================
# üìÇ Cargar documentos a Qdrant
# ==============================
docs = SimpleDirectoryReader(input_dir="./docs/").load_data()
index = VectorStoreIndex.from_documents(
    docs,
    embed_model=embed_model,
    storage_context=storage_context
)
chatbot_docs = index.as_chat_engine(llm=llm)

# ==============================
# ‚öôÔ∏è Configuraci√≥n SQL
# ==============================
sql_query_engine = NLSQLTableQueryEngine(
    sql_database=sql_database,
    tables=tablas_db,
    llm=llm,
    embed_model=embed_model 
)


# ==============================
# ü§ñ Chat h√≠brido
# ==============================
def hybrid_chatbot(user_input: str):
    try:
        tipo = detectar_intencion(user_input, llm)

        if tipo == "SQL":
            respuesta_sql = sql_query_engine.query(user_input)
            if not respuesta_sql:
                respuesta = "‚ùó No encontr√© resultados para esa consulta."
            else:
                respuesta = f"üìä Datos obtenidos:\n{respuesta_sql}"

        elif tipo == "DOCS":
            # Aqu√≠ ir√≠a tu motor de documentos Qdrant
             respuesta = chatbot_docs.chat(user_input).response

        else:
            logging.warning(f"Intenci√≥n no clara: {user_input}")
            respuesta = "‚ùì No estoy seguro de c√≥mo responder a eso. Lo estoy registrando para mejorar."

        # Logear QA
        #log_qa(user_input, respuesta)
        return respuesta

    except Exception as e:
        logging.error(f"Error al procesar la pregunta: {user_input} | {e}")
        return "‚ö†Ô∏è Hubo un error procesando tu pregunta. Se registr√≥ para revisi√≥n."


# ==============================
# üöÄ Bucle de conversaci√≥n
# ==============================
# if __name__ == "__main__":
#     print("ü§ñ Chatbot iniciado. Escribe 'exit' o 'salir' para terminar.")
#     while True:
#         user_input = input("\nPregunta: ")
#         if user_input.lower() in ["exit", "salir"]:
#             print("Hasta luego! üëã")
#             break
#         
#         respuesta = hybrid_chatbot(user_input)
#         print(f"\nRespuesta IA: {respuesta}\n")

